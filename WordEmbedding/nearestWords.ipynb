{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest words via embeddings\n",
    "Here is the prompt to generate this code in ChatGPT\n",
    "Write a python program using pytorch to create embeddings for a list of words. Then given the input of one of the words find the 5 closest words to it. \n",
    "\n",
    "Question - Are the embeddings normalized? ie are they of length 1 ?\n",
    "\n",
    "## Exercise in Class\n",
    "Add a capability to embed 5 phrases containing up to 30 words, \n",
    "then given a query select the phrase that might answer that query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.nn' from '/home/codespace/.local/lib/python3.12/site-packages/torch/nn/__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for the first two words:\n",
      "tensor([[-0.5814, -0.5345, -0.5858, -0.1236, -0.5404, -0.6483,  1.1738,  1.1161,\n",
      "          0.7136,  0.1037,  1.3802,  0.2885,  0.0857, -0.6413, -0.0583, -1.9639,\n",
      "          1.5322,  0.2474,  0.6956, -0.4548,  0.3738, -0.7276, -0.0718,  0.8499,\n",
      "         -1.2576, -0.2303,  1.9752,  1.7365,  0.2653,  0.0364, -0.6435, -0.3675,\n",
      "         -0.5705,  1.8230,  1.4834, -0.1357, -1.2909,  0.9535,  1.0271,  1.0067,\n",
      "          0.7270,  0.3991, -1.0937, -0.9313,  1.2438,  0.9947, -0.5409,  0.5476,\n",
      "         -0.8724, -0.0973],\n",
      "        [-0.2458,  0.0500,  0.2172,  1.4909, -0.6108, -1.7328, -0.7391, -0.2985,\n",
      "          0.8389,  0.0183,  1.5236,  1.1159,  0.7781,  0.3074, -1.5263,  0.7955,\n",
      "          0.1966, -0.6849,  0.2517, -0.5324,  0.0206, -1.3999, -0.5060, -0.4395,\n",
      "          0.7224,  1.1292, -1.6691, -0.6456, -0.5425, -0.2378, -0.8077,  2.6304,\n",
      "         -0.7133,  0.9567,  0.1927,  0.0066,  1.3816,  0.3905,  0.4137,  1.1556,\n",
      "         -0.2576,  0.1480, -0.3582, -0.0535,  1.5745,  0.7948,  1.1397, -0.6664,\n",
      "         -1.3388,  0.5718]], grad_fn=<SliceBackward0>)\n",
      "Embeddings for the last two words:\n",
      "tensor([[-0.4039,  0.2567, -1.5479,  0.0166,  1.7025, -0.8002, -0.2972,  0.2436,\n",
      "          1.2428,  0.4773,  0.3553,  1.9374,  0.3763,  0.3562, -1.5102,  0.4477,\n",
      "         -0.1237,  0.4425,  0.6110, -3.0775,  0.8613, -0.6860, -0.1298,  0.0606,\n",
      "         -1.3834, -1.3069, -0.1329, -0.3960, -0.6555, -1.5587, -0.4998, -0.4795,\n",
      "         -0.5783,  0.8436, -0.0798, -1.4885, -2.1104, -0.4831,  0.5471, -0.4238,\n",
      "          1.0388,  0.5111,  0.8079,  1.5519, -0.9111, -0.8143, -0.0983,  1.0668,\n",
      "          0.1663, -0.4881],\n",
      "        [-0.3736,  0.9169, -1.0356,  2.1010, -0.5294, -1.0767,  0.0259, -0.9359,\n",
      "         -0.9290, -0.1356, -1.1543, -1.9478, -1.0197, -0.6132, -1.3552,  1.6415,\n",
      "          0.8831,  1.5777, -1.2680,  0.5042,  0.5704, -1.6068, -2.1676, -0.6733,\n",
      "         -1.6401,  0.5936, -1.2249, -0.6029, -0.2577, -0.0927,  0.4722,  0.6255,\n",
      "          0.1175, -0.7991,  0.4860, -1.5253, -0.4477, -0.4410,  0.3065,  0.1671,\n",
      "          0.6404,  0.6763,  0.0136, -1.9293, -0.9528,  0.9800,  1.0438,  0.3191,\n",
      "         -0.7877,  0.3373]], grad_fn=<SliceBackward0>)\n",
      "Embedding for the word 'apple':\n",
      "tensor([-0.5814, -0.5345, -0.5858, -0.1236, -0.5404, -0.6483,  1.1738,  1.1161,\n",
      "         0.7136,  0.1037,  1.3802,  0.2885,  0.0857, -0.6413, -0.0583, -1.9639,\n",
      "         1.5322,  0.2474,  0.6956, -0.4548,  0.3738, -0.7276, -0.0718,  0.8499,\n",
      "        -1.2576, -0.2303,  1.9752,  1.7365,  0.2653,  0.0364, -0.6435, -0.3675,\n",
      "        -0.5705,  1.8230,  1.4834, -0.1357, -1.2909,  0.9535,  1.0271,  1.0067,\n",
      "         0.7270,  0.3991, -1.0937, -0.9313,  1.2438,  0.9947, -0.5409,  0.5476,\n",
      "        -0.8724, -0.0973], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a list of words (vocabulary)\n",
    "words = [\"apple\", \"banana\", \"orange\", \"pear\", \"peach\", \n",
    "         \"mango\", \"grape\", \"cherry\", \"berry\", \"melon\", \"robot\", \"machine\"]\n",
    "\n",
    "# Create mappings from word to index and index to word\n",
    "word2idx = {word: idx for idx, word in enumerate(words)}\n",
    "idx2word = {idx: word for idx, word in enumerate(words)}\n",
    "\n",
    "# Set embedding dimensions and create the embedding layer\n",
    "embedding_dim = 50\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(words), embedding_dim=embedding_dim)\n",
    "\n",
    "# Get embeddings for all words in the vocabulary\n",
    "# (This will be a matrix of shape [vocab_size, embedding_dim])\n",
    "embeddings = embedding_layer(torch.arange(len(words)))\n",
    "# print the embedding for the first two words\n",
    "print(\"Embeddings for the first two words:\")\n",
    "print(embeddings[:2])\n",
    "# print the embedding for the last two words\n",
    "print(\"Embeddings for the last two words:\")\n",
    "print(embeddings[-2:])\n",
    "# print the embedding for the word \"apple\"\n",
    "print(\"Embedding for the word 'apple':\")\n",
    "print(embeddings[word2idx[\"apple\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(word, top_k=7):\n",
    "    \"\"\"Finds the top_k closest words to the input word using cosine similarity.\"\"\"\n",
    "    if word not in word2idx:\n",
    "        print(f\"Word '{word}' not found in vocabulary.\")\n",
    "        return []\n",
    "    \n",
    "    # Get the embedding for the input word\n",
    "    word_index = word2idx[word]\n",
    "    word_embedding = embeddings[word_index]\n",
    "    \n",
    "    # Normalize all embeddings to unit length\n",
    "    normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    normalized_word_embedding = F.normalize(word_embedding, p=2, dim=0)\n",
    "    \n",
    "    # Compute cosine similarities: dot product between normalized vectors\n",
    "    similarities = torch.matmul(normalized_embeddings, normalized_word_embedding)\n",
    "    \n",
    "    # Get the indices of the top (top_k+1) similar words (including the word itself)\n",
    "    top_values, top_indices = torch.topk(similarities, top_k + 1)\n",
    "    \n",
    "    results = []\n",
    "    for value, idx in zip(top_values, top_indices):\n",
    "        # Skip the word itself\n",
    "        if idx.item() == word_index:\n",
    "            continue\n",
    "        results.append((idx2word[idx.item()], value.item()))\n",
    "        if len(results) == top_k:\n",
    "            break\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 7 words similar to 'banana':\n",
      "peach (cosine similarity: 0.2374)\n",
      "machine (cosine similarity: 0.1912)\n",
      "pear (cosine similarity: 0.1851)\n",
      "melon (cosine similarity: 0.1765)\n",
      "orange (cosine similarity: 0.0866)\n",
      "berry (cosine similarity: 0.0669)\n",
      "apple (cosine similarity: 0.0553)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_word = \"banana\"\n",
    "closest_words = find_closest(input_word)\n",
    "\n",
    "if closest_words:\n",
    "    print(f\"Top {len(closest_words)} words similar to '{input_word}':\")\n",
    "    for word, similarity in closest_words:\n",
    "        print(f\"{word} (cosine similarity: {similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 7 words similar to 'robot':\n",
      "grape (cosine similarity: 0.2172)\n",
      "apple (cosine similarity: 0.1565)\n",
      "peach (cosine similarity: 0.1386)\n",
      "berry (cosine similarity: 0.1303)\n",
      "mango (cosine similarity: 0.0714)\n",
      "cherry (cosine similarity: 0.0713)\n",
      "orange (cosine similarity: 0.0392)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_word = \"robot\"\n",
    "closest_words = find_closest(input_word)\n",
    "\n",
    "if closest_words:\n",
    "    print(f\"Top {len(closest_words)} words similar to '{input_word}':\")\n",
    "    for word, similarity in closest_words:\n",
    "        print(f\"{word} (cosine similarity: {similarity:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
